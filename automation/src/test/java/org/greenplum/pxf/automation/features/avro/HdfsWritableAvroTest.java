package org.greenplum.pxf.automation.features.avro;

import org.greenplum.pxf.automation.features.BaseFeature;
import org.greenplum.pxf.automation.structures.tables.pxf.ReadableExternalTable;
import org.greenplum.pxf.automation.structures.tables.pxf.WritableExternalTable;
import org.greenplum.pxf.automation.utils.system.ProtocolEnum;
import org.greenplum.pxf.automation.utils.system.ProtocolUtils;
import org.testng.annotations.Test;

import java.io.File;
import java.util.ArrayList;

public class HdfsWritableAvroTest extends BaseFeature {

    private ReadableExternalTable readableExternalTable;
    private ArrayList<File> filesToDelete;
    private final String[] avroPrimitiveTableCols = new String[]{
            "type_int int",
            "type_long bigint",
            "type_float real",
            "type_double float8",
            "type_string text",
            "type_bytes bytea",
            "type_boolean bool"
    };
    private final String[] avroComplexTableColsWritable = new String[]{
            "type_int int",
            "type_record struct",
            "type_enum_mood mood",
            "type_long_array BIGINT[]",
            "type_numeric_array NUMERIC(8,1)[]",
            "type_string_array TEXT[]"
    };
    private final String[] avroComplexTableColsReadable = new String[]{
            "type_int int",
            "type_record TEXT",
            "type_enum_mood TEXT",
            "type_long_array TEXT",
            "type_numeric_array TEXT",
            "type_string_array TEXT"
    };
    private String gpdbTable;
    private String hdfsPath;
    private String publicStage;
    private String resourcePath;
    private String fullTestPath;
    private ProtocolEnum protocol;

    @Override
    public void beforeClass() throws Exception {
        // path for storing data on HDFS (for processing by PXF)
        hdfsPath = hdfs.getWorkingDirectory() + "/writableAvro/";
        String absolutePath = getClass().getClassLoader().getResource("data").getPath();
        resourcePath = absolutePath + "/avro/";

        protocol = ProtocolUtils.getProtocol();
    }

    @Override
    public void beforeMethod() throws Exception {
        filesToDelete = new ArrayList<>();
        publicStage = "/tmp/publicstage/pxf/";
    }

    @Test(groups = {"features", "gpdb", "hcfs", "security"})
    public void generateSchemaPrimitive() throws Exception {
        gpdbTable = "writable_avro_primitive_generate_schema";
        fullTestPath = hdfsPath + "generate_schema_primitive_types";
        exTable = new WritableExternalTable(gpdbTable + "_writable", avroPrimitiveTableCols, fullTestPath, "custom");
        exTable.setHost(pxfHost);
        exTable.setPort(pxfPort);
        exTable.setFormatter("pxfwritable_export");
        exTable.setProfile(protocol.value() + ":avro");

        readableExternalTable = new ReadableExternalTable(gpdbTable + "_readable", avroPrimitiveTableCols, fullTestPath, "custom");
        readableExternalTable.setHost(pxfHost);
        readableExternalTable.setPort(pxfPort);
        readableExternalTable.setFormatter("pxfwritable_import");
        readableExternalTable.setProfile(protocol.value() + ":avro");
        gpdb.createTableAndVerify(readableExternalTable);

        gpdb.createTableAndVerify(exTable);

        insertPrimitives(gpdbTable);

        publicStage += "generateSchemaPrimitive/";
        // fetch all the segment-generated avro files and make them into json records
        // confirm that the lines generated by the segments match what we expect
        fetchAndVerifyAvroHcfsFiles("primitives.json");

        // check using GPDB readable external table that what went into HCFS is correct
        runTincTest("pxf.features.hdfs.writable.avro.primitives_generate_schema.runTest");
    }

    @Test(groups = {"features", "gpdb", "hcfs", "security"})
    public void generateSchemaComplex() throws Exception {
        gpdbTable = "writable_avro_complex_generate_schema";
        createComplexTypes();
        fullTestPath = hdfsPath + "generate_schema_complex_types";
        exTable = new WritableExternalTable(gpdbTable + "_writable", avroComplexTableColsWritable, fullTestPath, "custom");
        exTable.setHost(pxfHost);
        exTable.setPort(pxfPort);
        exTable.setFormatter("pxfwritable_export");
        exTable.setProfile(protocol.value() + ":avro");

        readableExternalTable = new ReadableExternalTable(gpdbTable + "_readable", avroComplexTableColsReadable, fullTestPath, "custom");
        readableExternalTable.setHost(pxfHost);
        readableExternalTable.setPort(pxfPort);
        readableExternalTable.setFormatter("pxfwritable_import");
        readableExternalTable.setProfile(protocol.value() + ":avro");
        gpdb.createTableAndVerify(readableExternalTable);

        gpdb.createTableAndVerify(exTable);

        insertComplex(gpdbTable);

        publicStage += "generateSchemaComplex/";
        // fetch all the segment-generated avro files and make them into json records
        // confirm that the lines generated by the segments match what we expect
        fetchAndVerifyAvroHcfsFiles("complex_records.json");

        // check using GPDB readable external table that what went into HCFS is correct
        runTincTest("pxf.features.hdfs.writable.avro.complex_generate_schema.runTest");
    }

    @Test(groups = {"features", "gpdb", "hcfs", "security"})
    public void userProvidedSchemaFileOnHcfsPrimitive() throws Exception {
        gpdbTable = "writable_avro_primitive_user_provided_schema_on_hcfs";
        fullTestPath = hdfsPath + "primitive_user_provided_schema_on_hcfs";
        exTable = new WritableExternalTable(gpdbTable + "_writable", avroPrimitiveTableCols, fullTestPath, "custom");
        exTable.setHost(pxfHost);
        exTable.setPort(pxfPort);
        exTable.setFormatter("pxfwritable_export");
        exTable.setProfile(protocol.value() + ":avro");

        readableExternalTable = new ReadableExternalTable(gpdbTable + "_readable", avroPrimitiveTableCols, fullTestPath, "custom");
        readableExternalTable.setHost(pxfHost);
        readableExternalTable.setPort(pxfPort);
        readableExternalTable.setFormatter("pxfwritable_import");
        readableExternalTable.setProfile(protocol.value() + ":avro");
        gpdb.createTableAndVerify(readableExternalTable);

        String schemaPath = hdfsPath.replaceFirst("/$", "_schema/primitives_no_union.avsc");
        // copy a schema file to HCFS that has no UNION types, just the raw underlying types.
        // the Avro files should thus be different from those without user-provided schema
        hdfs.copyFromLocal(resourcePath + "primitives_no_union.avsc", schemaPath);

        schemaPath = "/" + schemaPath;
        // if not on HCFS, get rid of bucket from hdfsPath, then append path to user-provided schema
        if (protocol != ProtocolEnum.HDFS) {
            schemaPath = schemaPath.replaceFirst("^/[^/]+(/.*)", "$1");
        }
        exTable.setExternalDataSchema(schemaPath);
        gpdb.createTableAndVerify(exTable);

        insertPrimitives(gpdbTable);

        publicStage += "userProvidedSchemaFileOnHcfsPrimitive/";
        // fetch all the segment-generated avro files and make them into json records
        // confirm that the lines generated by the segments match what we expect
        fetchAndVerifyAvroHcfsFiles("primitives_no_union.json");

        // check using GPDB readable external table that what went into HCFS is correct
        runTincTest("pxf.features.hdfs.writable.avro.primitives_user_provided_schema_on_hcfs.runTest");
    }

    @Test(groups = {"features", "gpdb", "hcfs", "security"})
    public void userProvidedSchemaFileOnClasspathComplex() throws Exception {
        createComplexTypes();
        gpdbTable = "writable_avro_complex_user_schema_on_classpath";
        fullTestPath = hdfsPath + "complex_user_schema_on_classpath";
        exTable = new WritableExternalTable(gpdbTable + "_writable",
                avroComplexTableColsWritable,
                fullTestPath,
                "custom");
        exTable.setHost(pxfHost);
        exTable.setPort(pxfPort);
        exTable.setFormatter("pxfwritable_export");
        exTable.setProfile(protocol.value() + ":avro");

        readableExternalTable = new ReadableExternalTable(gpdbTable + "_readable",
                avroComplexTableColsReadable,
                fullTestPath,
                "custom");
        readableExternalTable.setHost(pxfHost);
        readableExternalTable.setPort(pxfPort);
        readableExternalTable.setFormatter("pxfwritable_import");
        readableExternalTable.setProfile(protocol.value() + ":avro");
        gpdb.createTableAndVerify(readableExternalTable);

        // copy a schema file to PXF's classpath on cluster that has no UNION types, just the raw underlying types.
        // the Avro files should thus be different from those without user-provided schema
        cluster.copyFileToNodes(new File(resourcePath + "complex_no_union.avro").getAbsolutePath(),
                cluster.getPxfConfLocation(),
                false, false);
        exTable.setExternalDataSchema("complex_no_union.avro");
        gpdb.createTableAndVerify(exTable);

        insertComplex(gpdbTable);

        publicStage += "userProvidedSchemaFileOnClasspathComplex/";
        // fetch all the segment-generated avro files and make them into json records
        // confirm that the lines generated by the segments match what we expect
        fetchAndVerifyAvroHcfsFiles("complex_no_union.json");

        // check using GPDB readable external table that what went into HCFS is correct
        runTincTest("pxf.features.hdfs.writable.avro.complex_user_provided_schema_on_classpath.runTest");
    }

    @Test(groups = {"features", "gpdb", "hcfs", "security"})
    public void nullValues() throws Exception {
        gpdbTable = "writable_avro_null_values";
        createComplexTypes();
        fullTestPath = hdfsPath + "null_values";
        exTable = new WritableExternalTable(gpdbTable + "_writable", avroComplexTableColsWritable, fullTestPath, "custom");
        exTable.setHost(pxfHost);
        exTable.setPort(pxfPort);
        exTable.setFormatter("pxfwritable_export");
        exTable.setProfile(protocol.value() + ":avro");

        readableExternalTable = new ReadableExternalTable(gpdbTable + "_readable", avroComplexTableColsReadable, fullTestPath, "custom");
        readableExternalTable.setHost(pxfHost);
        readableExternalTable.setPort(pxfPort);
        readableExternalTable.setFormatter("pxfwritable_import");
        readableExternalTable.setProfile(protocol.value() + ":avro");
        gpdb.createTableAndVerify(readableExternalTable);

        gpdb.createTableAndVerify(exTable);

        insertComplexWithNulls(gpdbTable);

        publicStage += "nullValues/";
        // fetch all the segment-generated avro files and make them into json records
        // confirm that the lines generated by the segments match what we expect
        fetchAndVerifyAvroHcfsFiles("null_values.json");

        // check using GPDB readable external table that what went into HCFS is correct
        runTincTest("pxf.features.hdfs.writable.avro.null_values.runTest");
    }

    @Override
    protected void afterMethod() throws Exception {
        super.afterMethod();
        if (ProtocolUtils.getPxfTestDebug().equals("true")) {
            return;
        }
        for (File file : filesToDelete) {
            file.delete();
        }
        dropComplexTypes();
    }

    private void createComplexTypes() throws Exception {
        dropComplexTypes();
        gpdb.runQuery("CREATE TYPE mood AS ENUM ('sad', 'happy')");
        gpdb.runQuery("CREATE TYPE struct AS (b boolean, i int)");
    }

    private void dropComplexTypes() throws Exception {
        gpdb.runQuery("DROP TYPE IF EXISTS struct CASCADE", true, false);
        gpdb.runQuery("DROP TYPE IF EXISTS mood CASCADE", true, false);
    }

    private void insertPrimitives(String exTable) throws Exception {
        gpdb.runQuery("INSERT INTO " + exTable + "_writable " + "SELECT " +
                "i," +                                             // type_int
                "i*100000000000," +                                // type_long
                "i+1.0001," +                                      // type_float
                "i*100000.0001," +                                 // type_double
                "format('row_%s',i::varchar(255))," +              // type_string
                "format('bytes for %s',i::varchar(255))::bytea," + // type_bytes
                "CASE WHEN (i%2) = 0 THEN TRUE ELSE FALSE END " +  // type_boolean
                "from generate_series(1, 100) s(i);");
    }

    private void insertComplex(String gpdbTable) throws Exception {
        gpdb.runQuery("INSERT INTO " + gpdbTable + "_writable " + " SELECT " +
                "i," +
                "format('(%s, %s)',CASE WHEN (i%2) = 0 THEN FALSE ELSE TRUE END,(i*2)::varchar(255))::struct," +
                "CASE WHEN (i%2) = 0 THEN 'sad' ELSE 'happy' END::mood," +
                "format('{%s,%s,%s}',i::varchar(255),(i*10)::varchar(255),(i*100)::varchar(255))::BIGINT[]," +
                "format('{%s,%s,%s}',(i*1.0001)::varchar(255),((i*10.00001)*10)::varchar(255),((i*100.000001)*100)::varchar(255))::NUMERIC(8,1)[]," +
                "format('{\"item %s\",\"item %s\",\"item %s\"}',((i-1)*10)::varchar(255),(i*10)::varchar(255),((i+1)*10)::varchar(255))::TEXT[]" +
                "from generate_series(1, 100) s(i);");
    }

    private void insertComplexWithNulls(String gpdbTable) throws Exception {
        gpdb.runQuery("INSERT INTO " + gpdbTable + "_writable " + " SELECT " +
                "i," +
                "format('(%s, %s)',CASE WHEN (i%2) = 0 THEN FALSE ELSE TRUE END,(i*2)::varchar(255))::struct," +
                "CASE WHEN (i%3) = 0 THEN 'sad' WHEN (i%2) = 0 THEN 'happy' ELSE NULL END::mood," +
                "format('{%s,%s,%s}',i::varchar(255),(i*10)::varchar(255),(i*100)::varchar(255))::BIGINT[]," +
                "format('{%s,%s,%s}',(i*1.0001)::varchar(255),((i*10.00001)*10)::varchar(255),((i*100.000001)*100)::varchar(255))::NUMERIC(8,1)[]," +
                "format('{\"item %s\",\"item %s\",\"item %s\"}',((i-1)*10)::varchar(255),(i*10)::varchar(255),((i+1)*10)::varchar(255))::TEXT[]" +
                "from generate_series(1, 100) s(i);");
    }

    private void fetchAndVerifyAvroHcfsFiles(String compareFile) throws Exception {
        int cnt = 0;
        for (String srcPath : hdfs.list(fullTestPath)) {
            final String fileName = "file_" + cnt++;
            final String filePath = publicStage + fileName;
            filesToDelete.add(new File(filePath + ".avro"));
            filesToDelete.add(new File(publicStage + "." + fileName + ".avro.crc"));
            hdfs.copyToLocal(srcPath, filePath + ".avro");
            hdfs.writeJsonFileFromAvro("file://" + filePath + ".avro", filePath + ".json");
            filesToDelete.add(new File(filePath + ".json"));
        }
        Process p = Runtime
                .getRuntime()
                .exec(new String[]{
                        "bash",
                        "-c",
                        "diff <(cat " + publicStage + "file_*.json | sort -d) " + resourcePath + compareFile
                });
        p.waitFor();
        assert p.exitValue() == 0;
    }
}
